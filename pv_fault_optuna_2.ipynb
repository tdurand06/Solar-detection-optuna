{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+cGqythYgIVtrEritciaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdurand06/Solar-detection-optuna/blob/main/pv_fault_optuna_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVnBiT2JHE5r",
        "outputId": "493d834e-d072-4725-9abc-c803be296d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(1373798, 6)\n",
            "(1373798, 1)\n"
          ]
        }
      ],
      "source": [
        "from scipy.io import loadmat\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_file_path1 = '/content/drive/MyDrive/Solar/dataset_amb.mat'\n",
        "drive_file_path2 = '/content/drive/MyDrive/Solar/dataset_elec.mat'\n",
        "\n",
        "amb_data = loadmat(drive_file_path1)\n",
        "elec_data = loadmat(drive_file_path2)\n",
        "pd_elec_series=pd.Series(elec_data)\n",
        "pd_elec_series = pd_elec_series.drop([pd_elec_series.index[0],pd_elec_series.index[1],pd_elec_series.index[2]], axis=0)\n",
        "df_elec = pd.DataFrame({\n",
        "    name: values[0] for name, values in pd_elec_series.items()\n",
        "})\n",
        "\n",
        "pd_amb_series = pd.Series(amb_data)\n",
        "pd_amb_series = pd_amb_series.drop([pd_amb_series.index[0], pd_amb_series.index[1], pd_amb_series.index[2]])\n",
        "def process_values(values):\n",
        "    # If it's like irr or pvt ([[1.3729, 1.3604, ...]])\n",
        "    if len(values[0]) > 1:\n",
        "        return values[0]\n",
        "    # If it's like f_nv ([[0], [0], [0], ...])\n",
        "    else:\n",
        "        return [val[0] for val in values]\n",
        "\n",
        "df_amb = pd.DataFrame({\n",
        "    name: process_values(values)\n",
        "    for name, values in pd_amb_series.items()\n",
        "})\n",
        "X = pd.DataFrame({\n",
        "    'idc1': df_elec['idc1'],\n",
        "    'idc2': df_elec['idc2'],\n",
        "    'vdc1': df_elec['vdc1'],\n",
        "    'vdc2': df_elec['vdc2'],\n",
        "    'irr': df_amb['irr'],\n",
        "    'pvt': df_amb['pvt']\n",
        "})\n",
        "y = pd.DataFrame({\n",
        "    'f_nv': df_amb['f_nv']\n",
        "})\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "class PVFaultDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "        if isinstance(y, pd.DataFrame):\n",
        "            y = y.values\n",
        "        self.y = torch.tensor(y, dtype=torch.int64).squeeze()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def preprocess_data_subsample_three_splits(X, y, test_size=0.15, val_size=0.15):\n",
        "    class_counts = np.unique(y, return_counts=True)[1]\n",
        "    min_count = np.min(class_counts)\n",
        "    X_train_balanced = []\n",
        "    y_train_balanced = []\n",
        "\n",
        "    for i in np.unique(y):\n",
        "        # Use boolean indexing to extract, use values.ravel() to match the dimensions.\n",
        "        mask = y.values.ravel() == i\n",
        "        X_class = X[mask]\n",
        "        y_class = y[mask]\n",
        "        X_class_resampled, y_class_resampled = resample(X_class, y_class, n_samples=min_count, random_state=18)\n",
        "        X_train_balanced.append(X_class_resampled)\n",
        "        y_train_balanced.append(y_class_resampled)\n",
        "        # transform list back into (1373798, 6) (1373798, 1) respectively\n",
        "    X_trained_balanced = np.concatenate(X_train_balanced)\n",
        "    y_trained_balanced = np.concatenate(y_train_balanced)\n",
        "    #First split\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_trained_balanced, y_trained_balanced, test_size=test_size, random_state=42)\n",
        "    # Second split: separate validation set\n",
        "    val_size_adjusted = val_size / (1 - test_size)  # Adjust validation size\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,\n",
        "                                                      test_size=val_size_adjusted,\n",
        "                                                      random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_train_val = scaler.transform(X_train_val)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    train_dataset = PVFaultDataset(X_train, y_train)\n",
        "    val_dataset = PVFaultDataset(X_val, y_val)\n",
        "    train_val_dataset = PVFaultDataset(X_train_val, y_train_val)\n",
        "    test_dataset = PVFaultDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2, shuffle=False)\n",
        "    train_val_loader = DataLoader(train_val_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, num_workers=2, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, train_val_loader, test_loader, scaler\n",
        "\n",
        "class PVFaultClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate1=None, dropout_rate2=None):\n",
        "        super(PVFaultClassifier, self).__init__()\n",
        "        layers = [\n",
        "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        if dropout_rate1 is not None:\n",
        "            layers.append(nn.Dropout(dropout_rate1))\n",
        "        layers += [\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        if dropout_rate2 is not None:\n",
        "            layers.append(nn.Dropout(dropout_rate2))\n",
        "        layers.append(nn.Linear(64, 5))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TrainingMetrics Class\n",
        " Responsible for tracking, storing, and managing all metrics\n",
        "during the model training process. This class maintains a history of performance\n",
        "metrics and provides methods to update and retrieve this information.\n",
        "\n",
        "Key responsibilities:\n",
        "- Store training and validation metrics (losses and accuracies)\n",
        "- Track the best model state based on validation loss\n",
        "- Store prediction and target data for confusion matrix generation\n",
        "- Provide methods to retrieve metrics in formats suitable for visualization\n"
      ],
      "metadata": {
        "id": "BfeATa-3IYi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TrainingMetrics:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize metrics storage containers\"\"\"\n",
        "        # Store all metrics in dictionaries for consistency and easy access\n",
        "        self.metrics = {\n",
        "            'train_losses': [],       # Training loss history\n",
        "            'val_losses': [],         # Validation loss history\n",
        "            'train_accuracies': [],   # Training accuracy history\n",
        "            'val_accuracies': []      # Validation accuracy history\n",
        "        }\n",
        "\n",
        "        # Store data needed for confusion matrix and additional analysis\n",
        "        self.data = {\n",
        "            'pred_train': [],         # Predictions on training data\n",
        "            'target_train': [],       # Ground truth for training data\n",
        "            'pred_val': [],           # Predictions on validation data\n",
        "            'target_val': []          # Ground truth for validation data\n",
        "        }\n",
        "\n",
        "        # Track best model state and validation loss\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc, predictions=None, targets=None):\n",
        "        \"\"\"\n",
        "        Update metrics after an epoch of training\n",
        "\n",
        "        Args:\n",
        "            train_loss: The training loss for this epoch\n",
        "            train_acc: The training accuracy for this epoch\n",
        "            val_loss: The validation loss for this epoch\n",
        "            val_acc: The validation accuracy for this epoch\n",
        "            predictions: Model predictions on validation data (optional)\n",
        "            targets: True labels for validation data (optional)\n",
        "\n",
        "        Returns:\n",
        "            bool: True if this model has the best validation loss so far\n",
        "        \"\"\"\n",
        "        # Update metric history\n",
        "        self.metrics['train_losses'].append(train_loss)\n",
        "        self.metrics['val_losses'].append(val_loss)\n",
        "        self.metrics['train_accuracies'].append(train_acc)\n",
        "        self.metrics['val_accuracies'].append(val_acc)\n",
        "\n",
        "        # Store predictions and targets if provided\n",
        "        if predictions is not None and targets is not None:\n",
        "            self.data['pred_val'] = predictions\n",
        "            self.data['target_val'] = targets\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def update_confusion_matrix_data(self, pred_train, target_train, pred_val, target_val):\n",
        "        \"\"\"\n",
        "        Store all data needed for confusion matrix visualization\n",
        "\n",
        "        Args:\n",
        "            pred_train: Predictions on training data\n",
        "            target_train: Ground truth for training data\n",
        "            pred_val: Predictions on validation data\n",
        "            target_val: Ground truth for validation data\n",
        "        \"\"\"\n",
        "        self.data['pred_train'] = pred_train\n",
        "        self.data['target_train'] = target_train\n",
        "        self.data['pred_val'] = pred_val\n",
        "        self.data['target_val'] = target_val\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"\n",
        "        Create a summary of training results\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics of the training process\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'final_train_loss': self.metrics['train_losses'][-1] if self.metrics['train_losses'] else None,\n",
        "            'final_val_loss': self.metrics['val_losses'][-1] if self.metrics['val_losses'] else None,\n",
        "            'final_train_acc': self.metrics['train_accuracies'][-1] if self.metrics['train_accuracies'] else None,\n",
        "            'final_val_acc': self.metrics['val_accuracies'][-1] if self.metrics['val_accuracies'] else None,\n",
        "            'epochs_completed': len(self.metrics['train_losses'])\n",
        "        }"
      ],
      "metadata": {
        "id": "CDEGIzscHJzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ModelTrainer Class\n",
        "Manages the training process for PyTorch models.\n",
        "\n",
        "This class handles the entire training lifecycle including:\n",
        "- Running training and validation epochs\n",
        "- Computing and tracking metrics\n",
        "- Implementing early stopping\n",
        "- Managing the learning rate scheduler\n",
        "- Supporting Optuna trials for hyperparameter optimization\n",
        "\n",
        "The class is designed to work with any PyTorch model, loss function, and optimizer,\n",
        "making it highly reusable across different projects.\n",
        "\n"
      ],
      "metadata": {
        "id": "wIsEbsODIxAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install optuna_dashboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IapNc-_NJdrP",
        "outputId": "3b1eab1d-8446-491f-ff53-3c204147eff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Collecting optuna_dashboard\n",
            "  Downloading optuna_dashboard-0.17.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting bottle>=0.13.0 (from optuna_dashboard)\n",
            "  Downloading bottle-0.13.2-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: optuna>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from optuna_dashboard) (4.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optuna_dashboard) (24.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from optuna_dashboard) (1.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1.0->optuna_dashboard) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna_dashboard) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna_dashboard) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->optuna_dashboard) (3.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_dashboard) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_dashboard) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna_dashboard) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna_dashboard) (3.0.2)\n",
            "Downloading optuna_dashboard-0.17.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bottle-0.13.2-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bottle, optuna_dashboard\n",
            "Successfully installed bottle-0.13.2 optuna_dashboard-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Optional, Tuple, List\n",
        "import copy\n",
        "\n",
        "@dataclass\n",
        "class SchedulerConfig:\n",
        "    \"\"\"Simple configuration class for learning rate scheduler\"\"\"\n",
        "    mode: str = 'min'\n",
        "    factor: float = 0.1\n",
        "    patience: int = 5\n",
        "\n",
        "    def create_scheduler(self, optimizer):\n",
        "        \"\"\"Create a ReduceLROnPlateau scheduler with the specified configuration\"\"\"\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode=self.mode,\n",
        "            factor=self.factor,\n",
        "            patience=self.patience\n",
        "        )\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, criterion, optimizer,\n",
        "                 scheduler_config=None, device=None, patience=10):\n",
        "        \"\"\"\n",
        "        Initialize the model trainer\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch model to train\n",
        "            criterion: Loss function\n",
        "            optimizer: Optimization algorithm\n",
        "            scheduler_config: Configuration for learning rate scheduler (optional)\n",
        "            device: Device to run training on ('cuda' or 'cpu')\n",
        "            patience: Number of epochs to wait before early stopping\n",
        "        \"\"\"\n",
        "        # Set device if not provided\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize components\n",
        "        self.model = model.to(self.device)\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler_config.create_scheduler(optimizer) if scheduler_config else None\n",
        "        self.patience = patience\n",
        "        self.metrics = TrainingMetrics()\n",
        "\n",
        "        print(f\"ModelTrainer initialized with device: {self.device}\")\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"\n",
        "        Train for a single epoch\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for training data\n",
        "\n",
        "        Returns:\n",
        "            tuple: (avg_loss, accuracy, predictions, targets)\n",
        "        \"\"\"\n",
        "        self.model.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        # Iterate through batches\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # Move data to device\n",
        "            X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(X_batch)\n",
        "            loss = self.criterion(outputs, y_batch)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            targets.extend(y_batch.cpu().numpy())\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss and accuracy\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "        return avg_loss, accuracy, predictions, targets\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        \"\"\"\n",
        "        Validate the model\n",
        "\n",
        "        Args:\n",
        "            val_loader: DataLoader for validation data\n",
        "\n",
        "        Returns:\n",
        "            tuple: (avg_loss, accuracy, predictions, targets)\n",
        "        \"\"\"\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        # No gradient calculation during validation\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                # Move data to device\n",
        "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                predictions.extend(predicted.cpu().numpy())\n",
        "                targets.extend(y_batch.cpu().numpy())\n",
        "                total += y_batch.size(0)\n",
        "                correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss and accuracy\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "        return avg_loss, accuracy, predictions, targets\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs, trial=None):\n",
        "        \"\"\"\n",
        "        Complete training process with early stopping\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for training data\n",
        "            val_loader: DataLoader for validation data\n",
        "            num_epochs: Maximum number of epochs to train\n",
        "            trial: Optuna trial for hyperparameter optimization (optional)\n",
        "\n",
        "        Returns:\n",
        "            TrainingMetrics: Object containing training history\n",
        "        \"\"\"\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        print(f\"Starting training for {num_epochs} epochs\")\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            train_loss, train_acc, pred_train, target_train = self.train_epoch(train_loader)\n",
        "\n",
        "            # Validation phase\n",
        "            val_loss, val_acc, pred_val, target_val = self.validate(val_loader)\n",
        "\n",
        "            # Update metrics and check if best model\n",
        "            is_best = self.metrics.update(train_loss, train_acc, val_loss, val_acc, pred_val, target_val)\n",
        "\n",
        "            # Print progress\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            if self.scheduler is not None:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                self.scheduler.step(val_loss)\n",
        "                new_lr = self.optimizer.param_groups[0]['lr']\n",
        "                if new_lr != old_lr:\n",
        "                    print(f\"Learning rate adjusted: {old_lr:.6f} -> {new_lr:.6f}\")\n",
        "\n",
        "            # Optuna pruning\n",
        "            if trial:\n",
        "                trial.report(val_loss, epoch)\n",
        "                if trial.should_prune():\n",
        "                    print(\"Trial pruned by Optuna\")\n",
        "                    raise optuna.TrialPruned()\n",
        "\n",
        "            # Early stopping logic\n",
        "            if is_best:\n",
        "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= self.patience:\n",
        "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "        # Store final data for confusion matrix\n",
        "        self.metrics.update_confusion_matrix_data(pred_train, target_train, pred_val, target_val)\n",
        "\n",
        "        # Restore best model\n",
        "        if best_model_state:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "            self.metrics.best_model_state = best_model_state\n",
        "            print(\"Restored best model from training\")\n",
        "\n",
        "        # Print training summary\n",
        "        summary = self.metrics.get_summary()\n",
        "        print(f\"Best validation loss: {summary['best_val_loss']:.4f}\")\n",
        "\n",
        "        return self.metrics"
      ],
      "metadata": {
        "id": "bBj7KHHLIvzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# OptunaTrainer Class\n",
        "Manages hyperparameter optimization using Optuna.\n",
        "\n",
        "This class encapsulates the hyperparameter optimization process, including:\n",
        "- Defining the hyperparameter search space\n",
        "- Creating models with different hyperparameter configurations\n",
        "- Training and evaluating each model configuration\n",
        "- Optimizing to find the best parameters\n",
        "- Visualizing the optimization process with Optuna Dashboard\n",
        "\n",
        "The class is designed to work with any model class that accepts the defined\n",
        "hyperparameters and follows a consistent interface.\n"
      ],
      "metadata": {
        "id": "EefjuTglIviM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.trial import Trial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Type, Dict, Tuple\n",
        "import threading\n",
        "from datetime import datetime\n",
        "try:\n",
        "    from google.colab import output\n",
        "    from optuna_dashboard import run_server\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "class OptunaTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_class,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        hyperparameter_ranges=None,\n",
        "        n_trials=100,\n",
        "        n_jobs=2,\n",
        "        study_name=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Optuna trainer\n",
        "\n",
        "        Args:\n",
        "            model_class: The model class to optimize\n",
        "            train_loader: DataLoader for training data\n",
        "            val_loader: DataLoader for validation data\n",
        "            criterion: Loss function\n",
        "            hyperparameter_ranges: Dictionary defining the search space (optional)\n",
        "            n_trials: Number of trials to run\n",
        "            n_jobs: Number of parallel jobs\n",
        "            study_name: Name for the Optuna study\n",
        "        \"\"\"\n",
        "        # Initialize components\n",
        "        self.model_class = model_class\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.n_trials = n_trials\n",
        "        self.n_jobs = n_jobs\n",
        "        self.study_name = study_name or f\"optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        # Set default hyperparameter search space if not provided\n",
        "        self.hyperparameter_ranges = hyperparameter_ranges or {\n",
        "            'learning_rate': {\n",
        "                'type': 'float',\n",
        "                'low': 1e-5,\n",
        "                'high': 1e-1,\n",
        "                'log': True\n",
        "            },\n",
        "            'dropout_rate1': {\n",
        "                'type': 'float',\n",
        "                'low': 0.1,\n",
        "                'high': 0.7\n",
        "            },\n",
        "            'dropout_rate2': {\n",
        "                'type': 'float',\n",
        "                'low': 0.1,\n",
        "                'high': 0.7\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Initial configuration for first trial\n",
        "        self.initial_config = {\n",
        "            'learning_rate': 0.02,\n",
        "            'dropout_rate1': 0.2,\n",
        "            'dropout_rate2': 0.5\n",
        "        }\n",
        "\n",
        "        print(f\"OptunaTrainer initialized with {n_trials} trials and {n_jobs} parallel jobs\")\n",
        "\n",
        "    def create_model(self, trial):\n",
        "        \"\"\"\n",
        "        Create model and optimizer with trial parameters\n",
        "\n",
        "        Args:\n",
        "            trial: Optuna trial object\n",
        "\n",
        "        Returns:\n",
        "            tuple: (model, optimizer)\n",
        "        \"\"\"\n",
        "        # Create model with trial parameters\n",
        "        model = self.model_class(\n",
        "            dropout_rate1=trial.params['dropout_rate1'],\n",
        "            dropout_rate2=trial.params['dropout_rate2'],\n",
        "        )\n",
        "\n",
        "        # Create optimizer\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=trial.params['learning_rate'],\n",
        "        )\n",
        "\n",
        "        return model, optimizer\n",
        "\n",
        "    def objective(self, trial):\n",
        "        \"\"\"\n",
        "        Objective function for Optuna optimization\n",
        "\n",
        "        Args:\n",
        "            trial: Optuna trial object\n",
        "\n",
        "        Returns:\n",
        "            float: Validation loss to minimize\n",
        "        \"\"\"\n",
        "        print(f\"Starting trial {trial.number}\")\n",
        "\n",
        "        # Define hyperparameters for this trial\n",
        "        for param_name, param_config in self.hyperparameter_ranges.items():\n",
        "            if param_config['type'] == 'float':\n",
        "                trial.suggest_float(\n",
        "                    param_name,\n",
        "                    param_config['low'],\n",
        "                    param_config['high'],\n",
        "                    log=param_config.get('log', False)\n",
        "                )\n",
        "            elif param_config['type'] == 'int':\n",
        "                trial.suggest_int(\n",
        "                    param_name,\n",
        "                    param_config['low'],\n",
        "                    param_config['high'],\n",
        "                    step=param_config.get('step', 1),\n",
        "                    log=param_config.get('log', False)\n",
        "                )\n",
        "            elif param_config['type'] == 'categorical':\n",
        "                trial.suggest_categorical(param_name, param_config['choices'])\n",
        "\n",
        "        # Create model and optimizer\n",
        "        model, optimizer = self.create_model(trial)\n",
        "\n",
        "        # Create scheduler\n",
        "        scheduler_config = SchedulerConfig()\n",
        "\n",
        "        # Create trainer and run training\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        trainer = ModelTrainer(\n",
        "            model=model,\n",
        "            criterion=self.criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler_config=scheduler_config,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        try:\n",
        "            metrics = trainer.train(\n",
        "                train_loader=self.train_loader,\n",
        "                val_loader=self.val_loader,\n",
        "                num_epochs=20,\n",
        "                trial=trial\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial.number} finished with loss: {metrics.best_val_loss:.4f}\")\n",
        "            return metrics.best_val_loss\n",
        "\n",
        "        except optuna.TrialPruned:\n",
        "            print(f\"Trial {trial.number} pruned\")\n",
        "            raise\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the hyperparameter optimization\n",
        "\n",
        "        Returns:\n",
        "            optuna.Study: Study object with optimization results\n",
        "        \"\"\"\n",
        "        print(f\"Starting optimization with {self.n_trials} trials\")\n",
        "\n",
        "        # Create study\n",
        "        db_path = \"sqlite:///optuna.db\"\n",
        "        study = optuna.create_study(\n",
        "            direction='minimize',\n",
        "            pruner=optuna.pruners.MedianPruner(),\n",
        "            sampler=optuna.samplers.TPESampler(seed=42),\n",
        "            storage=db_path,\n",
        "            study_name=self.study_name,\n",
        "            load_if_exists=True\n",
        "        )\n",
        "\n",
        "        # Start with initial configuration\n",
        "        study.enqueue_trial(self.initial_config)\n",
        "\n",
        "        # Run optimization\n",
        "        study.optimize(self.objective, n_trials=self.n_trials, n_jobs=self.n_jobs)\n",
        "\n",
        "        # Log results\n",
        "        print(\"Optimization completed\")\n",
        "        print(f\"Best trial: {study.best_trial.number}\")\n",
        "        print(f\"Best value: {study.best_value}\")\n",
        "        print(f\"Best parameters: {study.best_params}\")\n",
        "\n",
        "        return study\n",
        "\n",
        "    def start_dashboard(self, port=8082):\n",
        "        \"\"\"\n",
        "        Start Optuna Dashboard in a separate thread\n",
        "\n",
        "        Args:\n",
        "            port: Port number for the dashboard\n",
        "        \"\"\"\n",
        "        if not COLAB_ENV:\n",
        "            print(\"Optuna Dashboard requires Google Colab environment\")\n",
        "            return\n",
        "\n",
        "        thread = threading.Thread(\n",
        "            target=run_server,\n",
        "            args=(\"sqlite:///optuna.db\",),\n",
        "            kwargs={\"port\": port, \"access_log_format\": \"\"}\n",
        "        )\n",
        "        thread.daemon = True  # Daemon threads exit when main thread exits\n",
        "        thread.start()\n",
        "        output.serve_kernel_port_as_iframe(port, path='/dashboard/')\n",
        "        print(f\"Optuna Dashboard started on port {port}\")"
      ],
      "metadata": {
        "id": "IH2Uo9QEJCOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ModelTrainingPipeline Class\n",
        "Orchestrates the entire model training workflow.\n",
        "\n",
        "This class coordinates the end-to-end machine learning pipeline:\n",
        "- Data preparation and splitting\n",
        "- Hyperparameter optimization\n",
        "- Final model training with best parameters\n",
        "- Model evaluation and saving\n",
        "\n",
        "It provides a high-level interface that abstracts away the complexity of the\n",
        "individual components while maintaining flexibility for customization.\n"
      ],
      "metadata": {
        "id": "K-Nl4b-eJCsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from datetime import datetime\n",
        "import optuna\n",
        "from typing import Dict, Tuple, Type, Optional\n",
        "\n",
        "class ModelTrainingPipeline:\n",
        "    def __init__(self, model_class, X, y, criterion=None, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the model training pipeline\n",
        "\n",
        "        Args:\n",
        "            model_class: The model class to train\n",
        "            X: Input features\n",
        "            y: Target labels\n",
        "            criterion: Loss function (default: CrossEntropyLoss)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        # Set random seeds for reproducibility\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "        # Initialize components\n",
        "        self.model_class = model_class\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.criterion = criterion or nn.CrossEntropyLoss()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize results storage\n",
        "        self.best_params = None\n",
        "        self.final_model = None\n",
        "        self.final_metrics = None\n",
        "        self.final_data = None\n",
        "        self.study = None\n",
        "\n",
        "        print(\"ModelTrainingPipeline initialized\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Input shape: {X.shape}\")\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"\n",
        "        Prepare data for training using the existing preprocessing function\n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_loader, val_loader, train_val_loader, test_loader, scaler)\n",
        "        \"\"\"\n",
        "        print(\"Preparing data\")\n",
        "        # Use your existing preprocessing function\n",
        "        train_loader, val_loader, train_val_loader, test_loader, scaler = preprocess_data_subsample_three_splits(\n",
        "            self.X, self.y\n",
        "        )\n",
        "\n",
        "        print(f\"Data prepared: {len(train_loader)} training batches, {len(val_loader)} validation batches\")\n",
        "        return train_loader, val_loader, train_val_loader, test_loader, scaler\n",
        "\n",
        "    def run_hyperparameter_optimization(self, n_trials=50, n_jobs=2, custom_ranges=None):\n",
        "        \"\"\"\n",
        "        Run hyperparameter optimization\n",
        "\n",
        "        Args:\n",
        "            n_trials: Number of trials to run\n",
        "            n_jobs: Number of parallel jobs\n",
        "            custom_ranges: Custom hyperparameter search space\n",
        "\n",
        "        Returns:\n",
        "            optuna.Study: Study object with optimization results\n",
        "        \"\"\"\n",
        "        print(f\"Starting hyperparameter optimization with {n_trials} trials\")\n",
        "\n",
        "        # Prepare data\n",
        "        train_loader, val_loader, _, _, _ = self.prepare_data()\n",
        "\n",
        "        # Create Optuna trainer\n",
        "        optuna_trainer = OptunaTrainer(\n",
        "            model_class=self.model_class,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=self.criterion,\n",
        "            hyperparameter_ranges=custom_ranges,\n",
        "            n_trials=n_trials,\n",
        "            n_jobs=n_jobs,\n",
        "            study_name=f\"optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        )\n",
        "\n",
        "        # Start dashboard (only works in Colab)\n",
        "        optuna_trainer.start_dashboard()\n",
        "\n",
        "        # Run optimization\n",
        "        self.study = optuna_trainer.run()\n",
        "\n",
        "        # Store best parameters\n",
        "        self.best_params = self.study.best_params\n",
        "        print(f\"Best parameters: {self.best_params}\")\n",
        "        print(f\"Best validation loss: {self.study.best_value:.4f}\")\n",
        "\n",
        "        return self.study\n",
        "\n",
        "    def train_final_model(self, num_epochs=50, custom_params=None):\n",
        "        \"\"\"\n",
        "        Train final model with best parameters\n",
        "\n",
        "        Args:\n",
        "            num_epochs: Number of epochs to train\n",
        "            custom_params: Custom parameters to use instead of best params\n",
        "\n",
        "        Returns:\n",
        "            tuple: (model, metrics)\n",
        "        \"\"\"\n",
        "        # Ensure we have parameters to use\n",
        "        params_to_use = custom_params or self.best_params\n",
        "        if params_to_use is None:\n",
        "            raise ValueError(\"No parameters available. Run hyperparameter optimization first or provide custom_params.\")\n",
        "\n",
        "        print(f\"Training final model for {num_epochs} epochs with parameters: {params_to_use}\")\n",
        "\n",
        "        # Prepare data - using train_val_loader for final training\n",
        "        _, _, train_val_loader, test_loader, _ = self.prepare_data()\n",
        "\n",
        "        # Initialize model with parameters\n",
        "        model = self.model_class(\n",
        "            dropout_rate1=params_to_use['dropout_rate1'],\n",
        "            dropout_rate2=params_to_use['dropout_rate2']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=params_to_use['learning_rate']\n",
        "        )\n",
        "\n",
        "        # Initialize scheduler\n",
        "        scheduler_config = SchedulerConfig()\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = ModelTrainer(\n",
        "            model=model,\n",
        "            criterion=self.criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler_config=scheduler_config,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        training_results = trainer.train(\n",
        "            train_loader=train_val_loader,\n",
        "            val_loader=test_loader,\n",
        "            num_epochs=num_epochs\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        self.final_model = model\n",
        "        self.final_metrics = training_results.metrics\n",
        "        self.final_data = training_results.data\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_acc, test_preds, test_targets = trainer.validate(test_loader)\n",
        "        print(f\"Final test loss: {test_loss:.4f}\")\n",
        "        print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        # Return results\n",
        "        final_results = {\n",
        "            'model': model,\n",
        "            'metrics': training_results.metrics,\n",
        "            'data': training_results.data,\n",
        "            'test_loss': test_loss,\n",
        "            'test_acc': test_acc,\n",
        "            'test_preds': test_preds,\n",
        "            'test_targets': test_targets,\n",
        "            'params': params_to_use\n",
        "        }\n",
        "\n",
        "        return model, final_results\n",
        "\n",
        "    def save_model(self, path=\"./saved_model.pt\"):\n",
        "        \"\"\"\n",
        "        Save the trained model to disk\n",
        "\n",
        "        Args:\n",
        "            path: Path to save the model\n",
        "        \"\"\"\n",
        "        if self.final_model is None:\n",
        "            raise ValueError(\"No model to save. Train a model first.\")\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
        "\n",
        "        # Prepare model info\n",
        "        model_info = {\n",
        "            'model_state_dict': self.final_model.state_dict(),\n",
        "            'parameters': self.best_params or {},\n",
        "            'metrics': {k: v[-1] for k, v in self.final_metrics.items()} if self.final_metrics else {},\n",
        "            'saved_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model_info, path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path=\"./saved_model.pt\"):\n",
        "        \"\"\"\n",
        "        Load a saved model\n",
        "\n",
        "        Args:\n",
        "            path: Path to the saved model\n",
        "\n",
        "        Returns:\n",
        "            model: Loaded model\n",
        "        \"\"\"\n",
        "        # Load model info\n",
        "        model_info = torch.load(path, map_location=self.device)\n",
        "\n",
        "        # Extract parameters\n",
        "        params = model_info.get('parameters', {})\n",
        "\n",
        "        # Create model\n",
        "        model = self.model_class(\n",
        "            dropout_rate1=params.get('dropout_rate1', 0.2),\n",
        "            dropout_rate2=params.get('dropout_rate2', 0.5)\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load model weights\n",
        "        model.load_state_dict(model_info['model_state_dict'])\n",
        "\n",
        "        # Update model and parameters\n",
        "        self.final_model = model\n",
        "        self.best_params = params\n",
        "\n",
        "        print(f\"Model loaded from {path}\")\n",
        "        print(f\"Parameters: {params}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "def run_complete_pipeline(X, y, model_class, n_trials=50, final_epochs=50):\n",
        "    \"\"\"\n",
        "    Run the complete model training pipeline\n",
        "\n",
        "    Args:\n",
        "        X: Input features\n",
        "        y: Target labels\n",
        "        model_class: Model class to train\n",
        "        n_trials: Number of hyperparameter optimization trials\n",
        "        final_epochs: Number of epochs for final model training\n",
        "\n",
        "    Returns:\n",
        "        pipeline: Trained pipeline object\n",
        "    \"\"\"\n",
        "    # Initialize the pipeline\n",
        "    pipeline = ModelTrainingPipeline(model_class, X, y)\n",
        "\n",
        "    # Run hyperparameter optimization\n",
        "    study = pipeline.run_hyperparameter_optimization(n_trials=n_trials)\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    final_model, final_results = pipeline.train_final_model(num_epochs=final_epochs)\n",
        "\n",
        "    # Save the model\n",
        "    pipeline.save_model(\"./best_model.pt\")\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "tQWgwIK9JDEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pipeline\n",
        "pipeline = ModelTrainingPipeline(PVFaultClassifier, X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg-Ht0ipRfeS",
        "outputId": "909acf58-7f11-4f5e-f0ec-1464f50d54b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelTrainingPipeline initialized\n",
            "Device: cpu\n",
            "Input shape: (1373798, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter optimization\n",
        "study = pipeline.run_hyperparameter_optimization(n_trials=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FcOTJ1W0RgEX",
        "outputId": "a2844738-8779-4fb9-b6b3-6060950c1fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter optimization with 20 trials\n",
            "Preparing data\n",
            "Data prepared: 657 training batches, 141 validation batches\n",
            "OptunaTrainer initialized with 20 trials and 2 parallel jobs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-9 (run_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8082, \"/dashboard/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "TypeError: run_server() got an unexpected keyword argument 'access_log_format'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna Dashboard started on port 8082\n",
            "Starting optimization with 20 trials\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:46:57,057] A new study created in RDB with name: optimization_20250225_224655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting trial 1\n",
            "Starting trial 0\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2795, Train Acc: 89.63%\n",
            "Val Loss: 0.1697, Val Acc: 95.56%\n",
            "Epoch [1/20]\n",
            "Train Loss: 1.5562, Train Acc: 36.93%\n",
            "Val Loss: 1.4932, Val Acc: 56.09%\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.4021, Train Acc: 51.54%\n",
            "Val Loss: 1.3045, Val Acc: 62.60%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.1675, Train Acc: 95.31%\n",
            "Val Loss: 0.1050, Val Acc: 97.40%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.1387, Train Acc: 95.98%\n",
            "Val Loss: 0.1065, Val Acc: 97.84%\n",
            "Epoch [3/20]\n",
            "Train Loss: 1.2039, Train Acc: 60.22%\n",
            "Val Loss: 1.1033, Val Acc: 67.20%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.1657, Train Acc: 95.50%\n",
            "Val Loss: 0.1710, Val Acc: 96.11%\n",
            "Epoch [4/20]\n",
            "Train Loss: 1.0219, Train Acc: 65.58%\n",
            "Val Loss: 0.9344, Val Acc: 72.16%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.1980, Train Acc: 94.57%\n",
            "Val Loss: 0.1035, Val Acc: 97.31%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.8851, Train Acc: 69.24%\n",
            "Val Loss: 0.8110, Val Acc: 72.84%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.1585, Train Acc: 95.54%\n",
            "Val Loss: 0.0862, Val Acc: 97.40%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.7836, Train Acc: 70.83%\n",
            "Val Loss: 0.7191, Val Acc: 72.91%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.1581, Train Acc: 95.59%\n",
            "Val Loss: 0.0918, Val Acc: 97.58%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.7077, Train Acc: 72.56%\n",
            "Val Loss: 0.6516, Val Acc: 73.78%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.1559, Train Acc: 95.49%\n",
            "Val Loss: 0.0960, Val Acc: 97.64%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.6542, Train Acc: 73.05%\n",
            "Val Loss: 0.6024, Val Acc: 76.91%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.1626, Train Acc: 95.28%\n",
            "Val Loss: 0.1330, Val Acc: 96.67%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.6123, Train Acc: 73.88%\n",
            "Val Loss: 0.5666, Val Acc: 87.02%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.1347, Train Acc: 95.97%\n",
            "Val Loss: 0.1280, Val Acc: 97.51%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.5819, Train Acc: 74.10%\n",
            "Val Loss: 0.5408, Val Acc: 91.93%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.1695, Train Acc: 94.98%\n",
            "Val Loss: 0.1194, Val Acc: 95.76%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.5608, Train Acc: 74.21%\n",
            "Val Loss: 0.5200, Val Acc: 92.24%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.2011, Train Acc: 94.19%\n",
            "Val Loss: 0.0897, Val Acc: 97.11%\n",
            "Learning rate adjusted: 0.020000 -> 0.002000\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.5445, Train Acc: 74.63%\n",
            "Val Loss: 0.5030, Val Acc: 92.80%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.1435, Train Acc: 95.91%\n",
            "Val Loss: 0.0748, Val Acc: 97.58%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.5282, Train Acc: 75.14%\n",
            "Val Loss: 0.4891, Val Acc: 93.04%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.1286, Train Acc: 96.49%\n",
            "Val Loss: 0.0675, Val Acc: 97.56%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.5112, Train Acc: 75.90%\n",
            "Val Loss: 0.4766, Val Acc: 93.27%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.1204, Train Acc: 96.49%\n",
            "Val Loss: 0.0638, Val Acc: 98.00%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.5006, Train Acc: 75.85%\n",
            "Val Loss: 0.4651, Val Acc: 93.31%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.1102, Train Acc: 96.84%\n",
            "Val Loss: 0.0611, Val Acc: 98.02%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.4916, Train Acc: 76.84%\n",
            "Val Loss: 0.4542, Val Acc: 92.40%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.1100, Train Acc: 96.85%\n",
            "Val Loss: 0.0593, Val Acc: 97.96%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.4790, Train Acc: 77.90%\n",
            "Val Loss: 0.4440, Val Acc: 93.24%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.1005, Train Acc: 97.01%\n",
            "Val Loss: 0.0582, Val Acc: 98.13%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.4686, Train Acc: 79.00%\n",
            "Val Loss: 0.4334, Val Acc: 91.84%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.1049, Train Acc: 97.04%\n",
            "Val Loss: 0.0564, Val Acc: 98.20%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.4576, Train Acc: 79.80%\n",
            "Val Loss: 0.4228, Val Acc: 93.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:49:38,465] Trial 0 finished with value: 0.05567428197774455 and parameters: {'learning_rate': 0.02, 'dropout_rate1': 0.2, 'dropout_rate2': 0.5}. Best is trial 0 with value: 0.05567428197774455.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0958, Train Acc: 97.15%\n",
            "Val Loss: 0.0557, Val Acc: 98.29%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0557\n",
            "Trial 0 finished with loss: 0.0557\n",
            "Starting trial 2\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:49:40,947] Trial 1 finished with value: 0.4120259438212036 and parameters: {'learning_rate': 1.9474110422394295e-05, 'dropout_rate1': 0.15920889589848272, 'dropout_rate2': 0.33462519013780456}. Best is trial 0 with value: 0.05567428197774455.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.4491, Train Acc: 80.83%\n",
            "Val Loss: 0.4120, Val Acc: 93.58%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.4120\n",
            "Trial 1 finished with loss: 0.4120\n",
            "Starting trial 3\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [1/20]\n",
            "Train Loss: 1.5286, Train Acc: 35.72%\n",
            "Val Loss: 1.3842, Val Acc: 42.56%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.4229, Train Acc: 83.59%\n",
            "Val Loss: 0.1664, Val Acc: 94.73%\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.2184, Train Acc: 52.95%\n",
            "Val Loss: 0.9859, Val Acc: 71.33%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.2003, Train Acc: 93.52%\n",
            "Val Loss: 0.1015, Val Acc: 96.24%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.8983, Train Acc: 65.19%\n",
            "Val Loss: 0.7089, Val Acc: 71.80%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.1641, Train Acc: 94.78%\n",
            "Val Loss: 0.0858, Val Acc: 97.22%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.7333, Train Acc: 69.60%\n",
            "Val Loss: 0.5972, Val Acc: 90.36%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.1393, Train Acc: 95.60%\n",
            "Val Loss: 0.0754, Val Acc: 97.53%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.6512, Train Acc: 72.06%\n",
            "Val Loss: 0.5427, Val Acc: 85.84%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.1251, Train Acc: 96.04%\n",
            "Val Loss: 0.0679, Val Acc: 97.64%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.6015, Train Acc: 73.65%\n",
            "Val Loss: 0.5095, Val Acc: 91.64%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.1225, Train Acc: 96.13%\n",
            "Val Loss: 0.0696, Val Acc: 97.51%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.5624, Train Acc: 75.17%\n",
            "Val Loss: 0.4841, Val Acc: 89.64%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.1132, Train Acc: 96.35%\n",
            "Val Loss: 0.0555, Val Acc: 98.07%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.5369, Train Acc: 77.05%\n",
            "Val Loss: 0.4635, Val Acc: 93.29%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.1101, Train Acc: 96.54%\n",
            "Val Loss: 0.0576, Val Acc: 98.07%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.5116, Train Acc: 78.50%\n",
            "Val Loss: 0.4416, Val Acc: 92.93%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.1116, Train Acc: 96.52%\n",
            "Val Loss: 0.0619, Val Acc: 97.71%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.4935, Train Acc: 81.09%\n",
            "Val Loss: 0.4216, Val Acc: 93.47%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.1022, Train Acc: 96.70%\n",
            "Val Loss: 0.0550, Val Acc: 98.29%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.4678, Train Acc: 83.12%\n",
            "Val Loss: 0.3951, Val Acc: 93.73%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.1036, Train Acc: 96.82%\n",
            "Val Loss: 0.0507, Val Acc: 98.36%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.4414, Train Acc: 85.07%\n",
            "Val Loss: 0.3654, Val Acc: 93.78%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.1039, Train Acc: 96.72%\n",
            "Val Loss: 0.0534, Val Acc: 98.27%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.4193, Train Acc: 86.77%\n",
            "Val Loss: 0.3340, Val Acc: 93.93%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0989, Train Acc: 96.88%\n",
            "Val Loss: 0.0550, Val Acc: 97.96%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.3839, Train Acc: 88.30%\n",
            "Val Loss: 0.3013, Val Acc: 94.00%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0970, Train Acc: 96.96%\n",
            "Val Loss: 0.0576, Val Acc: 98.38%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.3562, Train Acc: 89.50%\n",
            "Val Loss: 0.2738, Val Acc: 94.20%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0903, Train Acc: 97.23%\n",
            "Val Loss: 0.0507, Val Acc: 98.78%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.3335, Train Acc: 90.27%\n",
            "Val Loss: 0.2515, Val Acc: 94.22%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0959, Train Acc: 96.97%\n",
            "Val Loss: 0.0513, Val Acc: 97.82%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.3115, Train Acc: 90.98%\n",
            "Val Loss: 0.2324, Val Acc: 94.27%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0863, Train Acc: 97.25%\n",
            "Val Loss: 0.0474, Val Acc: 98.47%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.2938, Train Acc: 91.33%\n",
            "Val Loss: 0.2197, Val Acc: 94.29%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0880, Train Acc: 97.23%\n",
            "Val Loss: 0.0477, Val Acc: 98.33%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.2788, Train Acc: 91.77%\n",
            "Val Loss: 0.2086, Val Acc: 94.36%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0859, Train Acc: 97.42%\n",
            "Val Loss: 0.0559, Val Acc: 98.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:52:10,137] Trial 2 finished with value: 0.2017260121432602 and parameters: {'learning_rate': 5.083511067331046e-05, 'dropout_rate1': 0.15212788842563416, 'dropout_rate2': 0.648680610093231}. Best is trial 0 with value: 0.05567428197774455.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.2688, Train Acc: 91.96%\n",
            "Val Loss: 0.2017, Val Acc: 94.36%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.2017\n",
            "Trial 2 finished with loss: 0.2017\n",
            "Starting trial 4\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:52:16,026] Trial 3 finished with value: 0.04737417508702699 and parameters: {'learning_rate': 0.0042002343401415934, 'dropout_rate1': 0.5738753522116837, 'dropout_rate2': 0.48871062821317657}. Best is trial 3 with value: 0.04737417508702699.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0852, Train Acc: 97.39%\n",
            "Val Loss: 0.0480, Val Acc: 98.36%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0474\n",
            "Trial 3 finished with loss: 0.0474\n",
            "Starting trial 5\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [1/20]\n",
            "Train Loss: 1.6059, Train Acc: 21.54%\n",
            "Val Loss: 1.5752, Val Acc: 31.20%\n",
            "Epoch [1/20]\n",
            "Train Loss: 1.4865, Train Acc: 40.93%\n",
            "Val Loss: 1.2761, Val Acc: 54.89%\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.5475, Train Acc: 28.93%\n",
            "Val Loss: 1.5096, Val Acc: 47.82%\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.0207, Train Acc: 61.95%\n",
            "Val Loss: 0.8128, Val Acc: 88.73%\n",
            "Epoch [3/20]\n",
            "Train Loss: 1.4735, Train Acc: 37.73%\n",
            "Val Loss: 1.4295, Val Acc: 71.89%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.7517, Train Acc: 71.04%\n",
            "Val Loss: 0.6299, Val Acc: 92.60%\n",
            "Epoch [4/20]\n",
            "Train Loss: 1.3939, Train Acc: 44.10%\n",
            "Val Loss: 1.3402, Val Acc: 71.98%\n",
            "Epoch [5/20]\n",
            "Train Loss: 1.3105, Train Acc: 49.87%\n",
            "Val Loss: 1.2496, Val Acc: 83.27%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.6341, Train Acc: 74.87%\n",
            "Val Loss: 0.5426, Val Acc: 91.38%\n",
            "Epoch [6/20]\n",
            "Train Loss: 1.2251, Train Acc: 54.71%\n",
            "Val Loss: 1.1634, Val Acc: 84.73%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.5641, Train Acc: 76.52%\n",
            "Val Loss: 0.4932, Val Acc: 92.98%\n",
            "Epoch [7/20]\n",
            "Train Loss: 1.1460, Train Acc: 58.39%\n",
            "Val Loss: 1.0787, Val Acc: 73.49%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.5207, Train Acc: 78.92%\n",
            "Val Loss: 0.4582, Val Acc: 93.36%\n",
            "Epoch [8/20]\n",
            "Train Loss: 1.0718, Train Acc: 60.90%\n",
            "Val Loss: 0.9979, Val Acc: 73.13%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.4907, Train Acc: 80.90%\n",
            "Val Loss: 0.4285, Val Acc: 93.47%\n",
            "Epoch [9/20]\n",
            "Train Loss: 1.0051, Train Acc: 62.86%\n",
            "Val Loss: 0.9243, Val Acc: 72.78%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.4582, Train Acc: 83.35%\n",
            "Val Loss: 0.3977, Val Acc: 93.53%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.9453, Train Acc: 64.89%\n",
            "Val Loss: 0.8579, Val Acc: 72.51%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.4233, Train Acc: 86.21%\n",
            "Val Loss: 0.3635, Val Acc: 93.71%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.8949, Train Acc: 65.80%\n",
            "Val Loss: 0.8019, Val Acc: 72.44%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.3893, Train Acc: 88.44%\n",
            "Val Loss: 0.3274, Val Acc: 94.07%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.8507, Train Acc: 67.10%\n",
            "Val Loss: 0.7546, Val Acc: 72.38%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.3568, Train Acc: 90.14%\n",
            "Val Loss: 0.2921, Val Acc: 94.18%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.8165, Train Acc: 67.41%\n",
            "Val Loss: 0.7165, Val Acc: 72.44%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.3225, Train Acc: 91.36%\n",
            "Val Loss: 0.2626, Val Acc: 94.31%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.7812, Train Acc: 68.30%\n",
            "Val Loss: 0.6847, Val Acc: 72.53%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.3002, Train Acc: 92.13%\n",
            "Val Loss: 0.2376, Val Acc: 94.36%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.7598, Train Acc: 68.77%\n",
            "Val Loss: 0.6586, Val Acc: 72.58%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.2712, Train Acc: 92.81%\n",
            "Val Loss: 0.2190, Val Acc: 94.56%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.7349, Train Acc: 69.42%\n",
            "Val Loss: 0.6366, Val Acc: 73.09%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.2541, Train Acc: 93.16%\n",
            "Val Loss: 0.2029, Val Acc: 94.76%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.7160, Train Acc: 69.64%\n",
            "Val Loss: 0.6184, Val Acc: 75.93%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.2387, Train Acc: 93.36%\n",
            "Val Loss: 0.1935, Val Acc: 94.62%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.6905, Train Acc: 70.43%\n",
            "Val Loss: 0.6026, Val Acc: 78.56%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.2305, Train Acc: 93.54%\n",
            "Val Loss: 0.1845, Val Acc: 94.62%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.6855, Train Acc: 70.45%\n",
            "Val Loss: 0.5898, Val Acc: 85.04%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.2193, Train Acc: 93.83%\n",
            "Val Loss: 0.1785, Val Acc: 94.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:54:40,832] Trial 4 finished with value: 0.5785783867463998 and parameters: {'learning_rate': 1.2029879296935915e-05, 'dropout_rate1': 0.4182295733887287, 'dropout_rate2': 0.5786134364318898}. Best is trial 3 with value: 0.04737417508702699.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.6685, Train Acc: 70.68%\n",
            "Val Loss: 0.5786, Val Acc: 87.42%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.5786\n",
            "Trial 4 finished with loss: 0.5786\n",
            "Starting trial 6\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.2113, Train Acc: 93.85%\n",
            "Val Loss: 0.1725, Val Acc: 94.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:54:47,825] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]\n",
            "Train Loss: 1.6220, Train Acc: 20.90%\n",
            "Val Loss: 1.5901, Val Acc: 39.53%\n",
            "Trial pruned by Optuna\n",
            "Trial 6 pruned\n",
            "Starting trial 7\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:54:52,894] Trial 5 finished with value: 0.16919542228182158 and parameters: {'learning_rate': 4.985148325558795e-05, 'dropout_rate1': 0.18442564802804956, 'dropout_rate2': 0.4777069971073672}. Best is trial 3 with value: 0.04737417508702699.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.2052, Train Acc: 94.03%\n",
            "Val Loss: 0.1692, Val Acc: 94.78%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.1692\n",
            "Trial 5 finished with loss: 0.1692\n",
            "Starting trial 8\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2995, Train Acc: 89.39%\n",
            "Val Loss: 0.1383, Val Acc: 95.18%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.7571, Train Acc: 70.94%\n",
            "Val Loss: 0.7444, Val Acc: 71.31%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.1059, Train Acc: 96.51%\n",
            "Val Loss: 0.0765, Val Acc: 97.51%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.8788, Train Acc: 62.72%\n",
            "Val Loss: 0.6116, Val Acc: 73.84%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.0744, Train Acc: 97.57%\n",
            "Val Loss: 0.0599, Val Acc: 98.04%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.9710, Train Acc: 58.24%\n",
            "Val Loss: 0.7245, Val Acc: 71.69%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.0656, Train Acc: 97.79%\n",
            "Val Loss: 0.0507, Val Acc: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:55:23,963] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20]\n",
            "Train Loss: 1.0605, Train Acc: 52.70%\n",
            "Val Loss: 0.7793, Val Acc: 69.27%\n",
            "Trial pruned by Optuna\n",
            "Trial 8 pruned\n",
            "Starting trial 9\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0571, Train Acc: 98.04%\n",
            "Val Loss: 0.0538, Val Acc: 98.24%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.9821, Train Acc: 56.34%\n",
            "Val Loss: 0.8091, Val Acc: 72.87%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0536, Train Acc: 98.19%\n",
            "Val Loss: 0.0430, Val Acc: 98.53%\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.1339, Train Acc: 48.26%\n",
            "Val Loss: 1.2216, Val Acc: 41.33%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0539, Train Acc: 98.17%\n",
            "Val Loss: 0.0432, Val Acc: 98.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:55:48,116] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20]\n",
            "Train Loss: 1.1950, Train Acc: 44.22%\n",
            "Val Loss: 0.7393, Val Acc: 70.87%\n",
            "Trial pruned by Optuna\n",
            "Trial 9 pruned\n",
            "Starting trial 10\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0488, Train Acc: 98.31%\n",
            "Val Loss: 0.0403, Val Acc: 98.49%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.4291, Train Acc: 84.03%\n",
            "Val Loss: 0.1969, Val Acc: 93.80%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0446, Train Acc: 98.40%\n",
            "Val Loss: 0.0397, Val Acc: 98.62%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0452, Train Acc: 98.42%\n",
            "Val Loss: 0.0399, Val Acc: 98.78%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.3366, Train Acc: 88.68%\n",
            "Val Loss: 0.1669, Val Acc: 94.42%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0407, Train Acc: 98.51%\n",
            "Val Loss: 0.0401, Val Acc: 98.91%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.3118, Train Acc: 89.50%\n",
            "Val Loss: 0.1763, Val Acc: 94.93%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0399, Train Acc: 98.61%\n",
            "Val Loss: 0.0433, Val Acc: 98.73%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.3472, Train Acc: 88.93%\n",
            "Val Loss: 0.1951, Val Acc: 93.07%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0386, Train Acc: 98.64%\n",
            "Val Loss: 0.0322, Val Acc: 98.93%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.4133, Train Acc: 85.25%\n",
            "Val Loss: 0.2077, Val Acc: 94.36%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0366, Train Acc: 98.81%\n",
            "Val Loss: 0.0319, Val Acc: 99.00%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.3522, Train Acc: 88.08%\n",
            "Val Loss: 0.1575, Val Acc: 95.67%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0372, Train Acc: 98.78%\n",
            "Val Loss: 0.0516, Val Acc: 98.60%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.3579, Train Acc: 87.87%\n",
            "Val Loss: 0.2745, Val Acc: 92.69%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0375, Train Acc: 98.67%\n",
            "Val Loss: 0.0315, Val Acc: 99.09%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.3370, Train Acc: 89.07%\n",
            "Val Loss: 0.1784, Val Acc: 94.38%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0364, Train Acc: 98.68%\n",
            "Val Loss: 0.0319, Val Acc: 98.89%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.3487, Train Acc: 88.68%\n",
            "Val Loss: 0.2240, Val Acc: 93.96%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0333, Train Acc: 98.80%\n",
            "Val Loss: 0.0301, Val Acc: 98.96%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.4085, Train Acc: 87.15%\n",
            "Val Loss: 0.2733, Val Acc: 90.96%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0330, Train Acc: 98.87%\n",
            "Val Loss: 0.0300, Val Acc: 99.09%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.3602, Train Acc: 87.97%\n",
            "Val Loss: 0.2034, Val Acc: 94.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:57:18,172] Trial 7 finished with value: 0.028208779666183934 and parameters: {'learning_rate': 0.0018988577680075385, 'dropout_rate1': 0.12369830238514777, 'dropout_rate2': 0.17021212515398046}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0324, Train Acc: 98.79%\n",
            "Val Loss: 0.0282, Val Acc: 98.98%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0282\n",
            "Trial 7 finished with loss: 0.0282\n",
            "Starting trial 11\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.4163, Train Acc: 86.21%\n",
            "Val Loss: 0.2688, Val Acc: 92.42%\n",
            "Learning rate adjusted: 0.039170 -> 0.003917\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.6391, Train Acc: 73.53%\n",
            "Val Loss: 0.3407, Val Acc: 89.16%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.3914, Train Acc: 85.06%\n",
            "Val Loss: 0.3123, Val Acc: 88.29%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.2487, Train Acc: 92.11%\n",
            "Val Loss: 0.1707, Val Acc: 95.11%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.3730, Train Acc: 85.53%\n",
            "Val Loss: 0.3791, Val Acc: 83.00%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.1628, Train Acc: 94.66%\n",
            "Val Loss: 0.1387, Val Acc: 95.60%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.3724, Train Acc: 85.79%\n",
            "Val Loss: 0.3144, Val Acc: 83.44%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.1351, Train Acc: 95.40%\n",
            "Val Loss: 0.1080, Val Acc: 96.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:57:56,147] Trial 10 finished with value: 0.1574985284939514 and parameters: {'learning_rate': 0.03916999720367917, 'dropout_rate1': 0.4816443606039028, 'dropout_rate2': 0.14472006316170408}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20]\n",
            "Train Loss: 0.3700, Train Acc: 86.02%\n",
            "Val Loss: 0.3852, Val Acc: 83.07%\n",
            "Early stopping triggered after 16 epochs\n",
            "Restored best model from training\n",
            "Best validation loss: 0.1575\n",
            "Trial 10 finished with loss: 0.1575\n",
            "Starting trial 12\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.1085, Train Acc: 96.43%\n",
            "Val Loss: 0.0872, Val Acc: 97.71%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.4323, Train Acc: 83.38%\n",
            "Val Loss: 0.1862, Val Acc: 94.71%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0913, Train Acc: 97.05%\n",
            "Val Loss: 0.0717, Val Acc: 97.96%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.1760, Train Acc: 94.06%\n",
            "Val Loss: 0.1462, Val Acc: 95.04%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0827, Train Acc: 97.17%\n",
            "Val Loss: 0.0667, Val Acc: 98.16%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.1318, Train Acc: 95.40%\n",
            "Val Loss: 0.0932, Val Acc: 97.51%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0719, Train Acc: 97.52%\n",
            "Val Loss: 0.0646, Val Acc: 97.89%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.1028, Train Acc: 96.65%\n",
            "Val Loss: 0.0774, Val Acc: 97.62%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0668, Train Acc: 97.77%\n",
            "Val Loss: 0.0544, Val Acc: 98.24%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0890, Train Acc: 97.13%\n",
            "Val Loss: 0.0639, Val Acc: 98.02%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0643, Train Acc: 97.79%\n",
            "Val Loss: 0.0519, Val Acc: 98.31%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0785, Train Acc: 97.34%\n",
            "Val Loss: 0.0609, Val Acc: 97.98%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0602, Train Acc: 97.89%\n",
            "Val Loss: 0.0484, Val Acc: 98.42%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0720, Train Acc: 97.54%\n",
            "Val Loss: 0.0596, Val Acc: 98.09%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0572, Train Acc: 98.00%\n",
            "Val Loss: 0.0455, Val Acc: 98.42%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0718, Train Acc: 97.68%\n",
            "Val Loss: 0.0515, Val Acc: 98.20%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0602, Train Acc: 98.09%\n",
            "Val Loss: 0.0449, Val Acc: 98.62%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0676, Train Acc: 97.71%\n",
            "Val Loss: 0.0525, Val Acc: 98.40%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0551, Train Acc: 98.14%\n",
            "Val Loss: 0.0486, Val Acc: 98.42%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0659, Train Acc: 97.77%\n",
            "Val Loss: 0.0516, Val Acc: 98.36%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0499, Train Acc: 98.26%\n",
            "Val Loss: 0.0415, Val Acc: 98.56%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0617, Train Acc: 97.91%\n",
            "Val Loss: 0.0483, Val Acc: 98.40%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0504, Train Acc: 98.19%\n",
            "Val Loss: 0.0426, Val Acc: 98.49%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0596, Train Acc: 97.88%\n",
            "Val Loss: 0.0454, Val Acc: 98.58%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0488, Train Acc: 98.28%\n",
            "Val Loss: 0.0386, Val Acc: 98.87%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0466, Train Acc: 98.30%\n",
            "Val Loss: 0.0363, Val Acc: 98.76%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0572, Train Acc: 97.95%\n",
            "Val Loss: 0.0424, Val Acc: 98.67%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0468, Train Acc: 98.30%\n",
            "Val Loss: 0.0375, Val Acc: 98.76%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0556, Train Acc: 98.16%\n",
            "Val Loss: 0.0394, Val Acc: 98.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 22:59:52,768] Trial 11 finished with value: 0.03634491567776525 and parameters: {'learning_rate': 0.0005343131476735402, 'dropout_rate1': 0.32122257476605587, 'dropout_rate2': 0.1172155338538503}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0448, Train Acc: 98.50%\n",
            "Val Loss: 0.0366, Val Acc: 98.89%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0363\n",
            "Trial 11 finished with loss: 0.0363\n",
            "Starting trial 13\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0603, Train Acc: 97.80%\n",
            "Val Loss: 0.0471, Val Acc: 98.29%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.6295, Train Acc: 78.11%\n",
            "Val Loss: 0.2398, Val Acc: 94.42%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0595, Train Acc: 97.91%\n",
            "Val Loss: 0.0519, Val Acc: 97.71%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.2106, Train Acc: 93.48%\n",
            "Val Loss: 0.1620, Val Acc: 94.98%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0591, Train Acc: 97.94%\n",
            "Val Loss: 0.0418, Val Acc: 98.58%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.1617, Train Acc: 94.54%\n",
            "Val Loss: 0.1235, Val Acc: 95.73%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0549, Train Acc: 98.10%\n",
            "Val Loss: 0.0530, Val Acc: 98.04%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.1313, Train Acc: 95.55%\n",
            "Val Loss: 0.1011, Val Acc: 96.73%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0534, Train Acc: 98.09%\n",
            "Val Loss: 0.0518, Val Acc: 98.53%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.1112, Train Acc: 96.33%\n",
            "Val Loss: 0.0838, Val Acc: 97.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:00:33,103] Trial 12 finished with value: 0.039365038600401844 and parameters: {'learning_rate': 0.0018391579032621301, 'dropout_rate1': 0.3158610754626797, 'dropout_rate2': 0.3087159872113673}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0528, Train Acc: 98.12%\n",
            "Val Loss: 0.0403, Val Acc: 98.84%\n",
            "Learning rate adjusted: 0.001839 -> 0.000184\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0394\n",
            "Trial 12 finished with loss: 0.0394\n",
            "Starting trial 14\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0987, Train Acc: 96.73%\n",
            "Val Loss: 0.0750, Val Acc: 97.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:00:40,494] Trial 14 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]\n",
            "Train Loss: 0.6700, Train Acc: 74.91%\n",
            "Val Loss: 0.3225, Val Acc: 93.33%\n",
            "Trial pruned by Optuna\n",
            "Trial 14 pruned\n",
            "Starting trial 15\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0884, Train Acc: 97.02%\n",
            "Val Loss: 0.0690, Val Acc: 97.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:00:49,023] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]\n",
            "Train Loss: 0.7227, Train Acc: 73.23%\n",
            "Val Loss: 0.2971, Val Acc: 93.49%\n",
            "Trial pruned by Optuna\n",
            "Trial 15 pruned\n",
            "Starting trial 16\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0804, Train Acc: 97.35%\n",
            "Val Loss: 0.0654, Val Acc: 97.84%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2867, Train Acc: 89.27%\n",
            "Val Loss: 0.1093, Val Acc: 96.00%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0791, Train Acc: 97.40%\n",
            "Val Loss: 0.0635, Val Acc: 97.93%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.1148, Train Acc: 96.20%\n",
            "Val Loss: 0.0750, Val Acc: 97.31%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0713, Train Acc: 97.54%\n",
            "Val Loss: 0.0568, Val Acc: 98.07%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.0869, Train Acc: 97.05%\n",
            "Val Loss: 0.0639, Val Acc: 97.91%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0655, Train Acc: 97.74%\n",
            "Val Loss: 0.0517, Val Acc: 98.36%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.0791, Train Acc: 97.39%\n",
            "Val Loss: 0.0670, Val Acc: 97.82%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0647, Train Acc: 97.86%\n",
            "Val Loss: 0.0489, Val Acc: 98.42%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0759, Train Acc: 97.49%\n",
            "Val Loss: 0.0596, Val Acc: 97.91%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0628, Train Acc: 97.77%\n",
            "Val Loss: 0.0481, Val Acc: 98.44%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0671, Train Acc: 97.59%\n",
            "Val Loss: 0.0607, Val Acc: 98.18%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0559, Train Acc: 98.10%\n",
            "Val Loss: 0.0483, Val Acc: 98.60%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0657, Train Acc: 97.75%\n",
            "Val Loss: 0.0558, Val Acc: 98.16%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0551, Train Acc: 98.11%\n",
            "Val Loss: 0.0473, Val Acc: 98.58%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0627, Train Acc: 97.90%\n",
            "Val Loss: 0.0591, Val Acc: 98.27%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0539, Train Acc: 98.11%\n",
            "Val Loss: 0.0431, Val Acc: 98.62%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0609, Train Acc: 97.89%\n",
            "Val Loss: 0.0512, Val Acc: 98.18%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0512, Train Acc: 98.24%\n",
            "Val Loss: 0.0422, Val Acc: 98.76%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0544, Train Acc: 98.11%\n",
            "Val Loss: 0.0539, Val Acc: 98.60%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0494, Train Acc: 98.28%\n",
            "Val Loss: 0.0395, Val Acc: 98.82%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0530, Train Acc: 98.18%\n",
            "Val Loss: 0.0418, Val Acc: 98.60%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0488, Train Acc: 98.28%\n",
            "Val Loss: 0.0392, Val Acc: 98.71%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0533, Train Acc: 98.16%\n",
            "Val Loss: 0.0475, Val Acc: 98.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:02:24,987] Trial 13 finished with value: 0.039241078800145296 and parameters: {'learning_rate': 0.0004903679937331213, 'dropout_rate1': 0.2916384983533044, 'dropout_rate2': 0.12879059624968395}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0472, Train Acc: 98.45%\n",
            "Val Loss: 0.0402, Val Acc: 98.78%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0392\n",
            "Trial 13 finished with loss: 0.0392\n",
            "Starting trial 17\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0503, Train Acc: 98.24%\n",
            "Val Loss: 0.0483, Val Acc: 98.56%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2302, Train Acc: 91.51%\n",
            "Val Loss: 0.0832, Val Acc: 97.27%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0480, Train Acc: 98.29%\n",
            "Val Loss: 0.0402, Val Acc: 98.76%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.0937, Train Acc: 96.81%\n",
            "Val Loss: 0.0614, Val Acc: 98.00%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0446, Train Acc: 98.49%\n",
            "Val Loss: 0.0339, Val Acc: 98.69%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.0793, Train Acc: 97.39%\n",
            "Val Loss: 0.0554, Val Acc: 98.29%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0470, Train Acc: 98.45%\n",
            "Val Loss: 0.0376, Val Acc: 98.80%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.0739, Train Acc: 97.53%\n",
            "Val Loss: 0.0542, Val Acc: 98.40%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0462, Train Acc: 98.34%\n",
            "Val Loss: 0.0458, Val Acc: 98.31%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0654, Train Acc: 97.82%\n",
            "Val Loss: 0.0523, Val Acc: 98.27%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0451, Train Acc: 98.39%\n",
            "Val Loss: 0.0378, Val Acc: 98.60%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0631, Train Acc: 97.93%\n",
            "Val Loss: 0.0520, Val Acc: 98.40%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0432, Train Acc: 98.49%\n",
            "Val Loss: 0.0398, Val Acc: 98.98%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0596, Train Acc: 98.06%\n",
            "Val Loss: 0.0414, Val Acc: 98.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:03:25,101] Trial 16 finished with value: 0.03387931551304786 and parameters: {'learning_rate': 0.0036759612259284974, 'dropout_rate1': 0.2698292409801115, 'dropout_rate2': 0.21932164478838526}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0453, Train Acc: 98.45%\n",
            "Val Loss: 0.0456, Val Acc: 98.33%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0339\n",
            "Trial 16 finished with loss: 0.0339\n",
            "Starting trial 18\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0577, Train Acc: 98.12%\n",
            "Val Loss: 0.0536, Val Acc: 98.60%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0538, Train Acc: 98.23%\n",
            "Val Loss: 0.0527, Val Acc: 98.44%\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2152, Train Acc: 92.04%\n",
            "Val Loss: 0.0980, Val Acc: 97.53%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0561, Train Acc: 98.24%\n",
            "Val Loss: 0.0472, Val Acc: 98.53%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.1000, Train Acc: 96.73%\n",
            "Val Loss: 0.0684, Val Acc: 97.76%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0499, Train Acc: 98.29%\n",
            "Val Loss: 0.0429, Val Acc: 98.69%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.0859, Train Acc: 97.29%\n",
            "Val Loss: 0.0681, Val Acc: 98.18%\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0527, Train Acc: 98.28%\n",
            "Val Loss: 0.0430, Val Acc: 98.76%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.0794, Train Acc: 97.56%\n",
            "Val Loss: 0.0662, Val Acc: 97.93%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0530, Train Acc: 98.28%\n",
            "Val Loss: 0.0453, Val Acc: 98.73%\n",
            "Learning rate adjusted: 0.004788 -> 0.000479\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0797, Train Acc: 97.56%\n",
            "Val Loss: 0.0660, Val Acc: 97.91%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0385, Train Acc: 98.77%\n",
            "Val Loss: 0.0350, Val Acc: 98.93%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0777, Train Acc: 97.65%\n",
            "Val Loss: 0.0485, Val Acc: 98.47%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0353, Train Acc: 98.93%\n",
            "Val Loss: 0.0346, Val Acc: 99.02%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0703, Train Acc: 97.89%\n",
            "Val Loss: 0.0563, Val Acc: 98.47%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0330, Train Acc: 98.90%\n",
            "Val Loss: 0.0328, Val Acc: 99.07%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0711, Train Acc: 97.71%\n",
            "Val Loss: 0.0524, Val Acc: 98.22%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0321, Train Acc: 99.02%\n",
            "Val Loss: 0.0319, Val Acc: 98.89%\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0687, Train Acc: 97.96%\n",
            "Val Loss: 0.0507, Val Acc: 98.40%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0308, Train Acc: 98.97%\n",
            "Val Loss: 0.0314, Val Acc: 99.04%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0625, Train Acc: 98.16%\n",
            "Val Loss: 0.0612, Val Acc: 98.27%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0298, Train Acc: 99.04%\n",
            "Val Loss: 0.0318, Val Acc: 99.07%\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.0671, Train Acc: 97.96%\n",
            "Val Loss: 0.0529, Val Acc: 98.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:04:56,188] Trial 17 finished with value: 0.03083352790392047 and parameters: {'learning_rate': 0.0047876207188571945, 'dropout_rate1': 0.23971620535928692, 'dropout_rate2': 0.22575753792762931}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0302, Train Acc: 99.03%\n",
            "Val Loss: 0.0308, Val Acc: 99.02%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0308\n",
            "Trial 17 finished with loss: 0.0308\n",
            "Starting trial 19\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 20 epochs\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.0648, Train Acc: 98.00%\n",
            "Val Loss: 0.0689, Val Acc: 98.07%\n",
            "Learning rate adjusted: 0.009623 -> 0.000962\n",
            "Epoch [1/20]\n",
            "Train Loss: 0.2080, Train Acc: 91.80%\n",
            "Val Loss: 0.0847, Val Acc: 97.49%\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0480, Train Acc: 98.53%\n",
            "Val Loss: 0.0374, Val Acc: 98.89%\n",
            "Epoch [2/20]\n",
            "Train Loss: 0.0864, Train Acc: 97.25%\n",
            "Val Loss: 0.0835, Val Acc: 97.56%\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0405, Train Acc: 98.70%\n",
            "Val Loss: 0.0361, Val Acc: 98.69%\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.0806, Train Acc: 97.36%\n",
            "Val Loss: 0.0832, Val Acc: 97.51%\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0398, Train Acc: 98.71%\n",
            "Val Loss: 0.0339, Val Acc: 98.84%\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.0746, Train Acc: 97.49%\n",
            "Val Loss: 0.0933, Val Acc: 97.84%\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0364, Train Acc: 98.80%\n",
            "Val Loss: 0.0349, Val Acc: 98.80%\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.0722, Train Acc: 97.69%\n",
            "Val Loss: 0.0741, Val Acc: 97.78%\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0367, Train Acc: 98.75%\n",
            "Val Loss: 0.0340, Val Acc: 98.87%\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.0664, Train Acc: 97.72%\n",
            "Val Loss: 0.0538, Val Acc: 98.31%\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0371, Train Acc: 98.79%\n",
            "Val Loss: 0.0336, Val Acc: 98.93%\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.0650, Train Acc: 97.72%\n",
            "Val Loss: 0.0678, Val Acc: 97.64%\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0351, Train Acc: 98.86%\n",
            "Val Loss: 0.0327, Val Acc: 98.87%\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.0668, Train Acc: 97.81%\n",
            "Val Loss: 0.0625, Val Acc: 97.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:06:02,882] Trial 18 finished with value: 0.032427241211444975 and parameters: {'learning_rate': 0.009623066122972089, 'dropout_rate1': 0.20343056666888465, 'dropout_rate2': 0.2321409044901617}. Best is trial 7 with value: 0.028208779666183934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20]\n",
            "Train Loss: 0.0369, Train Acc: 98.79%\n",
            "Val Loss: 0.0324, Val Acc: 98.98%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0324\n",
            "Trial 18 finished with loss: 0.0324\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.0584, Train Acc: 97.99%\n",
            "Val Loss: 0.0587, Val Acc: 98.13%\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.0605, Train Acc: 98.03%\n",
            "Val Loss: 0.0785, Val Acc: 96.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:06:11,013] Trial 19 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20]\n",
            "Train Loss: 0.0556, Train Acc: 98.20%\n",
            "Val Loss: 0.0544, Val Acc: 98.27%\n",
            "Trial pruned by Optuna\n",
            "Trial 19 pruned\n",
            "Optimization completed\n",
            "Best trial: 7\n",
            "Best value: 0.028208779666183934\n",
            "Best parameters: {'learning_rate': 0.0018988577680075385, 'dropout_rate1': 0.12369830238514777, 'dropout_rate2': 0.17021212515398046}\n",
            "Best parameters: {'learning_rate': 0.0018988577680075385, 'dropout_rate1': 0.12369830238514777, 'dropout_rate2': 0.17021212515398046}\n",
            "Best validation loss: 0.0282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train final model with best parameters\n",
        "final_model, results = pipeline.train_final_model(num_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_cfFRb2RgTp",
        "outputId": "9ba0bcdb-22f4-4706-ffdf-54c9704f17f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model for 30 epochs with parameters: {'learning_rate': 0.0018988577680075385, 'dropout_rate1': 0.12369830238514777, 'dropout_rate2': 0.17021212515398046}\n",
            "Preparing data\n",
            "Data prepared: 657 training batches, 141 validation batches\n",
            "ModelTrainer initialized with device: cpu\n",
            "Starting training for 30 epochs\n",
            "Epoch [1/30]\n",
            "Train Loss: 0.3109, Train Acc: 88.48%\n",
            "Val Loss: 0.1020, Val Acc: 96.62%\n",
            "Epoch [2/30]\n",
            "Train Loss: 0.0933, Train Acc: 96.92%\n",
            "Val Loss: 0.0618, Val Acc: 97.93%\n",
            "Epoch [3/30]\n",
            "Train Loss: 0.0710, Train Acc: 97.66%\n",
            "Val Loss: 0.0483, Val Acc: 98.42%\n",
            "Epoch [4/30]\n",
            "Train Loss: 0.0620, Train Acc: 97.85%\n",
            "Val Loss: 0.0480, Val Acc: 98.36%\n",
            "Epoch [5/30]\n",
            "Train Loss: 0.0565, Train Acc: 98.11%\n",
            "Val Loss: 0.0351, Val Acc: 98.87%\n",
            "Epoch [6/30]\n",
            "Train Loss: 0.0517, Train Acc: 98.20%\n",
            "Val Loss: 0.0397, Val Acc: 98.89%\n",
            "Epoch [7/30]\n",
            "Train Loss: 0.0492, Train Acc: 98.33%\n",
            "Val Loss: 0.0360, Val Acc: 98.67%\n",
            "Epoch [8/30]\n",
            "Train Loss: 0.0447, Train Acc: 98.43%\n",
            "Val Loss: 0.0393, Val Acc: 98.91%\n",
            "Epoch [9/30]\n",
            "Train Loss: 0.0433, Train Acc: 98.43%\n",
            "Val Loss: 0.0325, Val Acc: 98.84%\n",
            "Epoch [10/30]\n",
            "Train Loss: 0.0437, Train Acc: 98.46%\n",
            "Val Loss: 0.0298, Val Acc: 99.02%\n",
            "Epoch [11/30]\n",
            "Train Loss: 0.0417, Train Acc: 98.49%\n",
            "Val Loss: 0.0359, Val Acc: 98.69%\n",
            "Epoch [12/30]\n",
            "Train Loss: 0.0382, Train Acc: 98.71%\n",
            "Val Loss: 0.0258, Val Acc: 99.31%\n",
            "Epoch [13/30]\n",
            "Train Loss: 0.0390, Train Acc: 98.65%\n",
            "Val Loss: 0.0530, Val Acc: 97.96%\n",
            "Epoch [14/30]\n",
            "Train Loss: 0.0385, Train Acc: 98.63%\n",
            "Val Loss: 0.0356, Val Acc: 98.78%\n",
            "Epoch [15/30]\n",
            "Train Loss: 0.0343, Train Acc: 98.86%\n",
            "Val Loss: 0.0261, Val Acc: 99.22%\n",
            "Epoch [16/30]\n",
            "Train Loss: 0.0336, Train Acc: 98.79%\n",
            "Val Loss: 0.0321, Val Acc: 98.67%\n",
            "Epoch [17/30]\n",
            "Train Loss: 0.0348, Train Acc: 98.83%\n",
            "Val Loss: 0.0243, Val Acc: 99.27%\n",
            "Epoch [18/30]\n",
            "Train Loss: 0.0326, Train Acc: 98.79%\n",
            "Val Loss: 0.0272, Val Acc: 99.22%\n",
            "Epoch [19/30]\n",
            "Train Loss: 0.0312, Train Acc: 98.92%\n",
            "Val Loss: 0.0270, Val Acc: 99.09%\n",
            "Epoch [20/30]\n",
            "Train Loss: 0.0311, Train Acc: 98.93%\n",
            "Val Loss: 0.0303, Val Acc: 99.07%\n",
            "Epoch [21/30]\n",
            "Train Loss: 0.0285, Train Acc: 98.95%\n",
            "Val Loss: 0.0245, Val Acc: 99.29%\n",
            "Epoch [22/30]\n",
            "Train Loss: 0.0306, Train Acc: 99.02%\n",
            "Val Loss: 0.0298, Val Acc: 99.27%\n",
            "Epoch [23/30]\n",
            "Train Loss: 0.0292, Train Acc: 98.97%\n",
            "Val Loss: 0.0203, Val Acc: 99.47%\n",
            "Epoch [24/30]\n",
            "Train Loss: 0.0284, Train Acc: 99.02%\n",
            "Val Loss: 0.0225, Val Acc: 99.42%\n",
            "Epoch [25/30]\n",
            "Train Loss: 0.0278, Train Acc: 99.01%\n",
            "Val Loss: 0.0230, Val Acc: 99.38%\n",
            "Epoch [26/30]\n",
            "Train Loss: 0.0286, Train Acc: 99.02%\n",
            "Val Loss: 0.0289, Val Acc: 99.18%\n",
            "Epoch [27/30]\n",
            "Train Loss: 0.0314, Train Acc: 98.93%\n",
            "Val Loss: 0.0236, Val Acc: 99.38%\n",
            "Epoch [28/30]\n",
            "Train Loss: 0.0288, Train Acc: 98.94%\n",
            "Val Loss: 0.0326, Val Acc: 99.04%\n",
            "Epoch [29/30]\n",
            "Train Loss: 0.0264, Train Acc: 99.09%\n",
            "Val Loss: 0.0219, Val Acc: 99.33%\n",
            "Learning rate adjusted: 0.001899 -> 0.000190\n",
            "Epoch [30/30]\n",
            "Train Loss: 0.0210, Train Acc: 99.27%\n",
            "Val Loss: 0.0179, Val Acc: 99.58%\n",
            "Restored best model from training\n",
            "Best validation loss: 0.0179\n",
            "Final test loss: 0.0179\n",
            "Final test accuracy: 99.58%\n"
          ]
        }
      ]
    }
  ]
}